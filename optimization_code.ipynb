{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'proyecto'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproyecto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'proyecto'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from proyecto import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Mini-Batch Gradient Descent  (mejora 1)\"\"\"  \n",
    "import math\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=32):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    #Paso 1: Creamos versión shuffled del set de entrenamiento\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    #Paso 2: Partir la mezcla en minibatchs de tamaño 'mini_batch_size'\n",
    "    inc = mini_batch_size\n",
    "    num_complete_minibatch = math.floor(m / mini_batch_size)\n",
    "\n",
    "    for k in range(0,num_complete_minibatch):\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, k*inc : (k+1)*inc]\n",
    "        mini_batch_Y = shuffled_Y[:, k*inc : (k+1)*inc]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatch*inc : ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatch*inc : ]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "#Paso 3: Modelo de la propagación hacia delante\n",
    "\n",
    "def L_model_forward(X, parameters, dropout_prob=0, layers_drop=[0, 0, 0, 0, 0]):\n",
    "\n",
    "    caches = []\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "\n",
    "    keep_prob = 1 - dropout_prob\n",
    "    \n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "\n",
    "        if layers_drop[l] == 1:\n",
    "            D = np.random.rand(*A.shape)\n",
    "            D = (D < keep_prob).astype(int)\n",
    "            A = A * D\n",
    "            A = A/keep_prob\n",
    "            \n",
    "            cache = (cache , D)\n",
    "        else:\n",
    "            cache = (cache, None)\n",
    "\n",
    "        caches.append(cache)       \n",
    "\n",
    "\n",
    "    #Ultima capa, sigmoid:\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Momentum para Gradient Descent (mejora 2)\"\"\"\n",
    "#When using minibatches for update we make the update just with a subset of examples. Momentum takes in account past gradients to smooth the \n",
    "# update.\n",
    "\n",
    "#Step 1 - Inicializar velocity 'v'\n",
    "def initilize_velocity(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    v = {}\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "\n",
    "        v[\"dW\"+str(l)] = np.zeros((parameters[\"W\"+str(l)].shape))\n",
    "        v[\"db\"+str(l)] = np.zeros((parameters[\"b\"+str(l)].shape))\n",
    "\n",
    "    return v\n",
    "\n",
    "#Step 2 - Update parameters with momentum\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "\n",
    "        #Exponentially Weighted Averages\n",
    "        v[\"dW\"+str(l)] = beta * v[\"dW\"+str(l)] + (1-beta) * grads[\"dW\"+str(l)]\n",
    "        v[\"db\"+str(l)] = beta * v[\"db\"+str(l)] + (1-beta) * grads[\"db\"+str(l)]\n",
    "\n",
    "        #Update of parameters with momentum\n",
    "        parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - learning_rate * v[\"dW\"+str(l)]\n",
    "        parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - learning_rate * v[\"db\"+str(l)]\n",
    "\n",
    "    return parameters, v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adam Algorithm - Momentum + RMSProp (mejora 3)\"\"\"\n",
    "\n",
    "#Step 1: Initialize with Adam: v --> exponentially weighted average. s --> exponentially weighted average of the squares\n",
    "def initialize_parameters_adam(parameters):\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "\n",
    "        v[\"dW\" + str(l)] = np.zeros((parameters[\"W\"+str(l)].shape))\n",
    "        v[\"db\" + str(l)] = np.zeros((parameters[\"b\"+str(l)].shape))\n",
    "        s[\"dW\" + str(l)] = np.zeros((parameters[\"W\"+str(l)].shape))\n",
    "        s[\"db\" + str(l)] = np.zeros((parameters[\"b\"+str(l)].shape))\n",
    "        \n",
    "    return v, s\n",
    "\n",
    "#Step 2: Calculates the respective exponentially weighted average of past gradient before and after bias correction and updates the parameters\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, beta1, beta2, learning_rate, epsilon):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "\n",
    "        #exponential weighted average 'v' and squared 's'\n",
    "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1-beta1) * grads[\"dW\"+str(l)]\n",
    "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1-beta1) * grads[\"db\"+str(l)]\n",
    "\n",
    "        v_corrected[\"dW\"+str(l)] = v[\"dW\"+str(l)] / (1-np.power(beta1,t))\n",
    "        v_corrected[\"db\"+str(l)] = v[\"db\"+str(l)] / (1-np.power(beta1,t))\n",
    "\n",
    "        s[\"dW\"+str(l)] = beta2 * s[\"dW\"+str(l)] + (1 - beta2) * np.power(grads[\"dW\"+str(l)], 2)\n",
    "        s[\"db\"+str(l)] = beta2 * s[\"db\"+str(l)] + (1 - beta2) * np.power(grads[\"db\"+str(l)], 2)\n",
    "        \n",
    "        #with bias correction\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\"+str(l)] / (1-np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\"+str(l)] / (1-np.power(beta2,t))\n",
    "\n",
    "        #update\n",
    "        parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - learning_rate * (v_corrected[\"dW\"+str(l)] / (np.sqrt(s_corrected[\"dW\"+str(l)]) + epsilon))\n",
    "        parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - learning_rate * (v_corrected[\"db\"+str(l)] / (np.sqrt(s_corrected[\"db\"+str(l)]) + epsilon))\n",
    "\n",
    "    return parameters, v, s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Regularization -- Reduce Overfitting --> L2\"\"\"\n",
    "\n",
    "#Step 1: compute the cost\n",
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cross_entropy_cost = bce_compute(AL,Y)\n",
    "\n",
    "    L2_regularization_cost = 0\n",
    "\n",
    "    for key in parameters:\n",
    "        if 'W' in key:\n",
    "            L2_regularization_cost += np.sum(np.square(parameters[key]))\n",
    "\n",
    "\n",
    "    L2_regularization_cost = (lambd / (2 * m)) * L2_regularization_cost\n",
    "\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "    return  cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
